{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7643349,"sourceType":"datasetVersion","datasetId":4455013}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile lora_layer.py\n\nimport math\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# The base class for LoRA layers\nclass LoraLayer:\n    def __init__(\n        self,\n        in_features: int,  # The number of input features\n        out_features: int,  # The number of output features\n    ):\n        # Initializes dictionaries to store various parameters for each adapter in the layer\n        self.r = {}  # The rank of the low-rank matrix\n        self.lora_alpha = {}  # The scaling factor\n        self.scaling = {}  # The calculated scaling factor (lora_alpha / r)\n\n        # Dropout layers for each adapter\n        self.lora_dropout = nn.ModuleDict({})\n\n        # Weight matrices for the linear layers\n        self.lora_A = nn.ModuleDict({})\n        self.lora_B = nn.ModuleDict({})\n\n        # Weight matrices for the embedding layers\n        self.lora_embedding_A = nn.ParameterDict({})\n        self.lora_embedding_B = nn.ParameterDict({})\n\n        # Boolean flag indicating whether the weights have been merged\n        self.merged = False\n\n        # Boolean flag indicating whether the adapters are disabled\n        self.disable_adapters = False\n\n        # Stores the number of input and output features\n        self.in_features = in_features\n        self.out_features = out_features\n    \n    # Method to update the parameters of the layer with a new adapter\n    def update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):\n        # Updates the rank and scaling factor for the adapter\n        self.r[adapter_name] = r\n        self.lora_alpha[adapter_name] = lora_alpha\n\n        # If dropout rate is greater than 0, creates a dropout layer, otherwise creates an identity layer\n        if lora_dropout > 0.0:\n            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n        else:\n            lora_dropout_layer = nn.Identity()\n\n        # Updates the dropout layer for the adapter\n        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n\n        # If rank is greater than 0, creates trainable parameters for the adapter\n        if r > 0:\n            self.lora_A.update(nn.ModuleDict({adapter_name: nn.Linear(self.in_features, r, bias=False)}))\n            self.lora_B.update(nn.ModuleDict({adapter_name: nn.Linear(r, self.out_features, bias=False)}))\n            self.scaling[adapter_name] = lora_alpha / r\n\n        # If init_lora_weights is True, resets the parameters of the adapter\n        if init_lora_weights:\n            self.reset_lora_parameters(adapter_name)\n\n        # Moves the layer to the same device as the weight tensor\n        self.to(self.weight.device)\n\n     # Method to update the parameters of the embedding layer with a new adapter\n    def update_layer_embedding(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights):\n        # Updates the rank and scaling factor for the adapter\n        self.r[adapter_name] = r\n        self.lora_alpha[adapter_name] = lora_alpha\n\n        # If dropout rate is greater than 0, creates a dropout layer, otherwise creates an identity layer\n        if lora_dropout > 0.0:\n            lora_dropout_layer = nn.Dropout(p=lora_dropout)\n        else:\n            lora_dropout_layer = nn.Identity()\n\n        # Updates the dropout layer for the adapter\n        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n\n        # If rank is greater than 0, creates trainable parameters for the adapter\n        if r > 0:\n            self.lora_embedding_A.update(\n                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((r, self.in_features)))})\n            )\n            self.lora_embedding_B.update(\n                nn.ParameterDict({adapter_name: nn.Parameter(self.weight.new_zeros((self.out_features, r)))})\n            )\n            self.scaling[adapter_name] = lora_alpha / r\n\n        # If init_lora_weights is True, resets the parameters of the adapter\n        if init_lora_weights:\n            self.reset_lora_parameters(adapter_name)\n\n        # Moves the layer to the same device as the weight tensor\n        self.to(self.weight.device)\n\n    # Method to reset the parameters of an adapter\n    def reset_lora_parameters(self, adapter_name):\n        if adapter_name in self.lora_A.keys():\n            # initialize A the same way as the default for nn.Linear and B to zero\n            nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B[adapter_name].weight)\n        if adapter_name in self.lora_embedding_A.keys():\n            # initialize a the same way as the default for nn.linear and b to zero\n            nn.init.zeros_(self.lora_embedding_A[adapter_name])\n            nn.init.normal_(self.lora_embedding_B[adapter_name])\n\n# LoRA implemented in an Embedding layer\nclass Embedding(nn.Embedding, LoraLayer):\n    \"\"\"\n    The Embedding class is an extension of the PyTorch nn.Embedding class \n    and LoraLayer class to incorporate the LoRA method.\n    \"\"\"\n    def __init__(\n        self,\n        adapter_name: str,\n        num_embeddings: int,\n        embedding_dim: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        **kwargs,\n    ):\n        # Pop the init_lora_weights flag from kwargs\n        init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n\n        # Call the constructors of the parent classes\n        nn.Embedding.__init__(self, num_embeddings, embedding_dim, **kwargs)\n        LoraLayer.__init__(self, in_features=num_embeddings, out_features=embedding_dim)\n\n        # Freezing the pre-trained weight matrix\n        self.weight.requires_grad = False\n\n        # Reset the parameters of the Embedding layer and update it with the adapter\n        nn.Embedding.reset_parameters(self)\n        self.update_layer_embedding(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n\n        # Set the active adapter\n        self.active_adapter = adapter_name\n\n    # Separate low-rank approximation from original weight\n    def unmerge(self, mode: bool = True):\n        # If the weights are already unmerged, raise a warning\n        if not self.merged:\n            warnings.warn(\"Already unmerged. Nothing to do.\")\n            return\n        # If the rank of the active adapter is greater than 0, subtract the product of the LoRA weights\n        # from the weights of the embedding\n        if self.r[self.active_adapter] > 0:\n            self.weight.data -= (\n                transpose(\n                    self.lora_embedding_B[self.active_adapter] @ self.lora_embedding_A[self.active_adapter], True\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = False\n\n    # Merge low-rank approximation with original weights\n    def merge(self):\n        # If the weights are already merged, raise a warning\n        if self.merged:\n            warnings.warn(\"Already merged. Nothing to do.\")\n            return\n        # If the rank of the active adapter is greater than 0, add the product of the LoRA weights\n        # to the weights of the embedding\n        if self.r[self.active_adapter] > 0:\n            self.weight.data += (\n                transpose(\n                    self.lora_embedding_B[self.active_adapter] @ self.lora_embedding_A[self.active_adapter], True\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = True\n\n    # Defines the computation performed at every call.\n    def forward(self, x: torch.Tensor):\n        # If adapters are disabled and there is an active adapter with rank > 0 and it is merged\n        # Subtract the LoRA weights from the original weights and set merged to False\n        if self.disable_adapters:\n            if self.r[self.active.adapter] > 0 and self.merged:\n                self.weight.data -= (\n                    transpose(\n                        self.lora_embedding_B[self.active_adapter].weight\n                        @ self.lora_embedding_A[self.active_adapter].weight,\n                        True,\n                    )\n                    * self.scaling[self.active_adapter]\n                )\n                self.merged = False\n            # Forward pass with the original weights\n            return nn.Embedding.forward(self, x)\n\n        # If there is an active adapter with rank > 0 and it is not merged\n        elif self.r[self.active_adapter] > 0 and not self.merged:\n            result = nn.Embedding.forward(self, x)\n            # Compute the forward pass with the LoRA weights and add it to the result\n            if self.r[self.active_adapter] > 0:\n                after_A = F.embedding(\n                    x,\n                    self.lora_embedding_A[self.active_adapter].T,\n                    self.padding_idx,\n                    self.max_norm,\n                    self.norm_type,\n                    self.scale_grad_by_freq,\n                    self.sparse,\n                )\n                result += (after_A @ self.lora_embedding_B[self.active_adapter].T) * self.scaling[self.active_adapter]\n            return result\n        else:\n            return nn.Embedding.forward(self, x)\n\n\n# Lora is implemented in a dense (Linear) layer\nclass Linear(nn.Linear, LoraLayer):\n    \n    def __init__(\n        self,\n        adapter_name: str,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n        **kwargs,\n    ):\n        # Initialize weights for LoRA layer\n        init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n\n        # Initialize linear and LoRA layers\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        LoraLayer.__init__(self, in_features=in_features, out_features=out_features)\n\n        # Freezing the pre-trained weight matrix\n        self.weight.requires_grad = False\n\n        # Transpose the weight if the layer to replace stores weight like (fan_in, fan_out)\n        self.fan_in_fan_out = fan_in_fan_out\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.T\n\n        # Reset linear layer parameters and update LoRA layer\n        nn.Linear.reset_parameters(self)\n        self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n        self.active_adapter = adapter_name\n\n    def merge(self):\n        # Merge low-rank approximation with original weights\n        if self.active_adapter not in self.lora_A.keys():\n            return\n        if self.merged:\n            warnings.warn(\"Already merged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n\n            # TODO: Merge the LoRA parameters by adding the product of lora_B weights and lora_A weights (after transposing \n            # if necessary) to the original weights, scaled by the LoRA scaling factor. After this operation, set the merged\n            # flag to True.\n            \n            ### YOUR CODE HERE ###\n            self.weight.data +=  (\n                transpose(\n                    self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight,\n                    self.fan_in_fan_out,\n                )\n                * self.scaling[self.active_adapter]\n            )\n            \n            ### YOUR CODE HERE ###\n            self.merged = True\n\n    def unmerge(self):\n        # Separate low-rank approximation from original weights\n        if self.active_adapter not in self.lora_A.keys():\n            return\n        if not self.merged:\n            warnings.warn(\"Already unmerged. Nothing to do.\")\n            return\n        if self.r[self.active_adapter] > 0:\n            self.weight.data -= (\n                transpose(\n                    self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight,\n                    self.fan_in_fan_out,\n                )\n                * self.scaling[self.active_adapter]\n            )\n            self.merged = False\n\n    def forward(self, x: torch.Tensor):\n        previous_dtype = x.dtype\n        if self.active_adapter not in self.lora_A.keys():\n            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        if self.disable_adapters:\n            if self.r[self.active_adapter] > 0 and self.merged:\n                self.unmerge()\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        elif self.r[self.active_adapter] > 0 and not self.merged:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n            # Changing data type for ensuring consistency\n            x = x.to(self.lora_A[self.active_adapter].weight.dtype)\n            \n            # TODO: If the LoRA adapter is active and not merged, add the output of the LoRA layers to the result. This involves\n            # passing the input through lora_A, applying dropout, then passing it through lora_B. The output is scaled by the\n            # LoRA scaling factor and added to the result.\n            \n            ### YOUR CODE HERE ###\n            result += (\n                self.lora_B[self.active_adapter](\n                    self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n                )\n                * self.scaling[self.active_adapter]\n            )\n        \n        else:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        \n        # Reverting to the previous data type\n        result = result.to(previous_dtype)\n        return result\n    \ndef transpose(weight, fan_in_fan_out):\n    # Helper function to transpose weights if required\n    return weight.T if fan_in_fan_out else weight\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.019143Z","iopub.execute_input":"2024-02-19T04:56:04.019535Z","iopub.status.idle":"2024-02-19T04:56:04.040962Z","shell.execute_reply.started":"2024-02-19T04:56:04.019505Z","shell.execute_reply":"2024-02-19T04:56:04.040112Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Overwriting lora_layer.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile lora_model.py\n\nimport importlib\nimport inspect\nimport sys, os\n\nfrom contextlib import contextmanager\n\nimport math\nimport re\nimport copy\nimport warnings\nfrom dataclasses import asdict, dataclass, field\nfrom enum import Enum\nfrom typing import List, Optional, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom peft import (\n    PeftConfig,\n    LoraConfig,\n    get_peft_model_state_dict,\n) \n\nfrom transformers.pytorch_utils import Conv1D\nfrom transformers.utils import PushToHubMixin\n\nfrom accelerate import dispatch_model, infer_auto_device_map\nfrom accelerate.hooks import (\n    AlignDevicesHook,\n    add_hook_to_module,\n    remove_hook_from_submodules,\n)\nfrom accelerate.utils import get_balanced_memory\n\nimport bitsandbytes as bnb\n\nfrom lora_layer import LoraLayer, Embedding, Linear\n\nclass LoraModel(torch.nn.Module):\n\n    def __init__(self, model, config, adapter_name='default'):\n        super().__init__()\n        self.model = model\n        self.forward = self.model.forward\n        self.peft_config = config\n        self.add_adapter(adapter_name, self.peft_config[adapter_name])\n\n    def add_adapter(self, adapter_name, config=None):\n        if config is not None:\n            model_config = self.model.config.to_dict() if hasattr(self.model.config, \"to_dict\") else self.model.config\n            config = self._prepare_lora_config(config, model_config)\n            self.peft_config[adapter_name] = config\n        self._find_and_replace(adapter_name)\n        if len(self.peft_config) > 1 and self.peft_config[adapter_name].bias != \"none\":\n            raise ValueError(\n                \"LoraModel supports only 1 adapter with bias. When using multiple adapters, set bias to 'none' for all adapters.\"\n            )\n        mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)\n        if self.peft_config[adapter_name].inference_mode:\n            _freeze_adapter(self.model, adapter_name)\n\n    def _find_and_replace(self, adapter_name):\n        lora_config = self.peft_config[adapter_name]\n        loaded_in_4bit = getattr(self.model, \"is_loaded_in_4bit\", False)\n        loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)\n        if (loaded_in_4bit or loaded_in_8bit) and not is_bnb_available():\n            raise ImportError(\n                \"To use Lora with 8-bit or 4-bit quantization, please install the `bitsandbytes` package. \"\n                \"You can install it with `pip install bitsandbytes`.\"\n            )\n        is_target_modules_in_base_model = False\n        kwargs = {\n            \"r\": lora_config.r,\n            \"lora_alpha\": lora_config.lora_alpha,\n            \"lora_dropout\": lora_config.lora_dropout,\n            \"fan_in_fan_out\": lora_config.fan_in_fan_out,\n            \"init_lora_weights\": lora_config.init_lora_weights,\n        }\n        key_list = [key for key, _ in self.model.named_modules()]\n        for key in key_list:\n            if isinstance(lora_config.target_modules, str):\n                target_module_found = re.fullmatch(lora_config.target_modules, key)\n            else:\n                target_module_found = any(key.endswith(target_key) for target_key in lora_config.target_modules)\n            if target_module_found:\n                if not is_target_modules_in_base_model:\n                    is_target_modules_in_base_model = True\n                parent, target, target_name = _get_submodules(self.model, key)\n                if hasattr(target, \"bias\"):\n                    bias = target.bias is not None\n\n                if isinstance(target, LoraLayer):\n                    target.update_layer(\n                        adapter_name,\n                        lora_config.r,\n                        lora_config.lora_alpha,\n                        lora_config.lora_dropout,\n                        lora_config.init_lora_weights,\n                    )\n                else:\n                    if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):\n                        eightbit_kwargs = kwargs.copy()\n                        eightbit_kwargs.update(\n                            {\n                                \"has_fp16_weights\": target.state.has_fp16_weights,\n                                \"memory_efficient_backward\": target.state.memory_efficient_backward,\n                                \"threshold\": target.state.threshold,\n                                \"index\": target.index,\n                            }\n                        )\n                        new_module = Linear8bitLt(\n                            adapter_name, target.in_features, target.out_features, bias=bias, **eightbit_kwargs\n                        )\n                        \n                    elif isinstance(target, torch.nn.Embedding):\n                        embedding_kwargs = kwargs.copy()\n                        embedding_kwargs.pop(\"fan_in_fan_out\", None)\n                        in_features, out_features = target.num_embeddings, target.embedding_dim\n                        new_module = Embedding(adapter_name, in_features, out_features, **embedding_kwargs)\n                    else:\n                        if isinstance(target, torch.nn.Linear):\n                            in_features, out_features = target.in_features, target.out_features\n                            if kwargs[\"fan_in_fan_out\"]:\n                                warnings.warn(\n                                    \"fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. \"\n                                    \"Setting fan_in_fan_out to False.\"\n                                )\n                                kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = False\n                        elif isinstance(target, Conv1D):\n                            in_features, out_features = (\n                                target.weight.ds_shape if hasattr(target.weight, \"ds_shape\") else target.weight.shape\n                            )\n                            if not kwargs[\"fan_in_fan_out\"]:\n                                warnings.warn(\n                                    \"fan_in_fan_out is set to False but the target module is `Conv1D`. \"\n                                    \"Setting fan_in_fan_out to True.\"\n                                )\n                                kwargs[\"fan_in_fan_out\"] = lora_config.fan_in_fan_out = True\n                        else:\n                            raise ValueError(\n                                f\"Target module {target} is not supported. \"\n                                f\"Currently, only `torch.nn.Linear` and `Conv1D` are supported.\"\n                            )\n                        new_module = Linear(adapter_name, in_features, out_features, bias=bias, **kwargs)\n\n                    self._replace_module(parent, target_name, new_module, target)\n        if not is_target_modules_in_base_model:\n            raise ValueError(\n                f\"Target modules {lora_config.target_modules} not found in the base model. \"\n                f\"Please check the target modules and try again.\"\n            )\n\n    def _replace_module(self, parent_module, child_name, new_module, old_module):\n        setattr(parent_module, child_name, new_module)\n        new_module.weight = old_module.weight\n        if hasattr(old_module, \"bias\"):\n            if old_module.bias is not None:\n                new_module.bias = old_module.bias\n\n        if getattr(old_module, \"state\", None) is not None:\n            new_module.state = old_module.state\n            new_module.to(old_module.weight.device)\n\n        # dispatch to correct device\n        for name, module in new_module.named_modules():\n            if \"lora_\" in name:\n                module.to(old_module.weight.device)\n\n    def __getattr__(self, name: str):\n        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n        try:\n            return super().__getattr__(name)  # defer to nn.Module's logic\n        except AttributeError:\n            return getattr(self.model, name)\n\n    def get_peft_config_as_dict(self, inference: bool = False):\n        config_dict = {}\n        for key, value in self.peft_config.items():\n            config = {k: v.value if isinstance(v, Enum) else v for k, v in asdict(value).items()}\n            if inference:\n                config[\"inference_mode\"] = True\n        config_dict[key] = config\n        return config\n\n    def _set_adapter_layers(self, enabled=True):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                module.disable_adapters = False if enabled else True\n\n    def enable_adapter_layers(self):\n        self._set_adapter_layers(enabled=True)\n\n    def disable_adapter_layers(self):\n        self._set_adapter_layers(enabled=False)\n\n    def set_adapter(self, adapter_name):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                if module.merged:\n                    warnings.warn(\"Adapter cannot be set when the model is merged. Unmerging the model first.\")\n                    module.unmerge()\n                module.active_adapter = adapter_name\n\n    def merge_adapter(self):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                module.merge()\n\n    def unmerge_adapter(self):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                module.unmerge()\n\n    @staticmethod\n    def _prepare_lora_config(peft_config, model_config):\n        if peft_config.target_modules is None:\n            if model_config[\"model_type\"] not in TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:\n                raise ValueError(\"Please specify `target_modules` in `peft_config`\")\n            peft_config.target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config[\"model_type\"]]\n        if peft_config.inference_mode:\n            peft_config.merge_weights = True\n        return peft_config\n\n    def merge_and_unload(self):\n        r\"\"\"\n        This method merges the LoRa layers into the base model. This is needed if someone wants to use the base model\n        as a standalone model.\n        \"\"\"\n        if getattr(self.config, \"model_type\", None) == \"gpt2\":\n            raise ValueError(\"GPT2 models are not supported for merging LORA layers\")\n\n        if getattr(self.model, \"is_loaded_in_8bit\", False) or getattr(self.model, \"is_loaded_in_4bit\", False):\n            raise ValueError(\"Cannot merge LORA layers when the model is loaded in 8-bit mode\")\n\n        key_list = [key for key, _ in self.model.named_modules() if \"lora\" not in key]\n        for key in key_list:\n            try:\n                parent, target, target_name = _get_submodules(self.model, key)\n            except AttributeError:\n                continue\n            if isinstance(target, LoraLayer):\n                bias = target.bias is not None\n                new_module = torch.nn.Linear(target.in_features, target.out_features, bias=bias)\n                target.merge()\n                self._replace_module(parent, target_name, new_module, target)\n\n            # save any additional trainable modules part of `modules_to_save`\n            if isinstance(target, ModulesToSaveWrapper):\n                setattr(parent, target_name, target.modules_to_save[target.active_adapter])\n\n        return self.model\n\n    def add_weighted_adapter(self, adapters, weights, adapter_name):\n        if len({self.peft_config[adapter].r for adapter in adapters}) != 1:\n            raise ValueError(\"All adapters must have the same r value\")\n        self.peft_config[adapter_name] = self.peft_config[adapters[0]]\n        self.peft_config[adapter_name].lora_alpha = self.peft_config[adapters[0]].r\n        self._find_and_replace(adapter_name)\n        mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)\n        _freeze_adapter(self.model, adapter_name)\n        key_list = [key for key, _ in self.model.named_modules() if \"lora\" not in key]\n        for key in key_list:\n            _, target, _ = _get_submodules(self.model, key)\n            if isinstance(target, LoraLayer):\n                if adapter_name in target.lora_A:\n                    target.lora_A[adapter_name].weight.data = target.lora_A[adapter_name].weight.data * 0.0\n                    target.lora_B[adapter_name].weight.data = target.lora_B[adapter_name].weight.data * 0.0\n                    for adapter, weight in zip(adapters, weights):\n                        if adapter not in target.lora_A:\n                            continue\n                        target.lora_A[adapter_name].weight.data += (\n                            target.lora_A[adapter].weight.data * weight * target.scaling[adapter]\n                        )\n                        target.lora_B[adapter_name].weight.data += target.lora_B[adapter].weight.data * weight\n\n                elif adapter_name in target.lora_embedding_A:\n                    target.lora_embedding_A[adapter_name].data = target.lora_embedding_A[adapter_name].data * 0.0\n                    target.lora_embedding_B[adapter_name].data = target.lora_embedding_B[adapter_name].data * 0.0\n                    for adapter, weight in zip(adapters, weights):\n                        if adapter not in target.lora_embedding_A:\n                            continue\n                        target.lora_embedding_A[adapter_name].data += (\n                            target.lora_embedding_A[adapter].data * weight * target.scaling[adapter]\n                        )\n                        target.lora_embedding_B[adapter_name].data += target.lora_embedding_B[adapter].data * weight\n\n\nclass LoraModelForCasualLM(PushToHubMixin, torch.nn.Module):\n\n    def __init__(self, model, peft_config: PeftConfig, adapter_name=\"default\"):\n        super().__init__()\n        self.base_model = model\n        self.config = self.base_model.config\n        self.modules_to_save = None\n        self.peft_config = {}\n        self.active_adapter = adapter_name\n        self.peft_type = peft_config.peft_type\n        self.base_model_torch_dtype = getattr(model, \"dtype\", None)\n        self.peft_config[adapter_name] = peft_config\n        self.base_model = LoraModel(self.base_model, self.peft_config, adapter_name)\n        self.set_additional_trainable_modules(peft_config, adapter_name)\n        \n        self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\n\n    def save_pretrained(self, save_directory, **kwargs):\n        if os.path.isfile(save_directory):\n            raise ValueError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n        os.makedirs(save_directory, exist_ok=True)\n\n        for adapter_name, peft_config in self.peft_config.items():\n            # save only the trainable weights\n            output_state_dict = get_peft_model_state_dict(\n                self, state_dict=kwargs.get(\"state_dict\", None), adapter_name=adapter_name\n            )\n            output_dir = os.path.join(save_directory, adapter_name) if adapter_name != \"default\" else save_directory\n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(output_state_dict, os.path.join(output_dir, \"adapter_model.bin\"))\n\n            # save the config and change the inference mode to `True`\n            if peft_config.base_model_name_or_path is None:\n                peft_config.base_model_name_or_path = self.base_model.model.__dict__.get(\"name_or_path\", None)\n            inference_mode = peft_config.inference_mode\n            peft_config.inference_mode = True\n            peft_config.save_pretrained(output_dir)\n            peft_config.inference_mode = inference_mode\n\n    @classmethod\n    def from_pretrained(cls, model, model_id, adapter_name=\"default\", is_trainable=False, **kwargs):\n        # load the config\n        config = LoraConfig.from_pretrained(model_id, subfolder=kwargs.get(\"subfolder\", None))\n\n        if (getattr(model, \"hf_device_map\", None) is not None) and len(\n            set(model.hf_device_map.values()).intersection({\"cpu\", \"disk\"})\n        ) > 0:\n            remove_hook_from_submodules(model)\n\n\n        config.inference_mode = not is_trainable\n\n        model = LoraModelForCasualLM(model, config, adapter_name)\n        model.load_adapter(model_id, adapter_name, **kwargs)\n        return model\n\n    def print_trainable_parameters(self):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in self.named_parameters():\n            num_params = param.numel()\n            # if using DS Zero 3 and the weights are initialized empty\n            if num_params == 0 and hasattr(param, \"ds_numel\"):\n                num_params = param.ds_numel\n\n            all_param += num_params\n            if param.requires_grad:\n                trainable_params += num_params\n        print(\n            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n        )\n\n    def __getattr__(self, name: str):\n        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n        try:\n            return super().__getattr__(name)  # defer to nn.Module's logic\n        except AttributeError:\n            return getattr(self.base_model, name)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        return self.base_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            **kwargs,\n        )\n\n    @contextmanager\n    def disable_adapter(self):\n        \"\"\"\n        Disables the adapter module.\n        \"\"\"\n        try:\n            self.base_model.disable_adapter_layers()\n        finally:\n            self.base_model.enable_adapter_layers()\n\n    def get_base_model(self):\n        \"\"\"\n        Returns the base model.\n        \"\"\"\n        return self.base_model.model\n\n    def add_adapter(self, adapter_name, peft_config):\n        if peft_config.peft_type != self.peft_type:\n            raise ValueError(\n                f\"Cannot combine adapters with different peft types. \"\n                f\"Found {self.peft_type} and {peft_config.peft_type}.\"\n            )\n        self.peft_config[adapter_name] = peft_config\n        self.base_model.add_adapter(adapter_name, peft_config)\n        self.set_additional_trainable_modules(peft_config, adapter_name)\n\n    def set_additional_trainable_modules(self, peft_config, adapter_name):\n        if getattr(peft_config, \"modules_to_save\", None) is not None:\n            if self.modules_to_save is None:\n                self.modules_to_save = set(peft_config.modules_to_save)\n            else:\n                self.modules_to_save.update(peft_config.modules_to_save)\n            _set_trainable(self, adapter_name)\n            \n    def generate(self, **kwargs):\n        peft_config = self.active_peft_config\n        self.base_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation\n        if hasattr(self.base_model, \"model\"):\n            self.base_model.model.generation_config = self.generation_config\n        else:\n            self.base_model.generation_config = self.generation_config\n        try:\n            outputs = self.base_model.generate(**kwargs)\n        except:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            raise\n        else:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            return outputs\n\n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        peft_config = self.active_peft_config\n        model_kwargs = self.base_model_prepare_inputs_for_generation(*args, **kwargs)\n        if model_kwargs[\"past_key_values\"] is None:\n            inputs_embeds = self.word_embeddings(model_kwargs[\"input_ids\"])\n            prompts = self.get_prompt(batch_size=model_kwargs[\"input_ids\"].shape[0])\n            prompts = prompts.to(inputs_embeds.dtype)\n            model_kwargs[\"inputs_embeds\"] = torch.cat((prompts, inputs_embeds), dim=1)\n            model_kwargs[\"input_ids\"] = None\n\n        return model_kwargs\n    \n    def load_adapter(self, model_id, adapter_name, is_trainable=False, **kwargs):\n        if adapter_name not in self.peft_config:\n            # load the config\n            peft_config = LoraConfig.from_pretrained(model_id, subfolder=kwargs.get(\"subfolder\", None))\n            peft_config.inference_mode = not is_trainable\n            self.add_adapter(adapter_name, peft_config)\n\n        # load weights if any\n        path = os.path.join(model_id, kwargs[\"subfolder\"]) if kwargs.get(\"subfolder\", None) is not None else model_id\n\n        if os.path.exists(os.path.join(path, \"adapter_model.bin\")):\n            filename = os.path.join(path, \"adapter_model.bin\")\n        else:\n            raise ValueError(\n                f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. \"\n                f\"Please check that the file adapter.bin is present at {model_id}.\"\n            )\n\n        adapters_weights = torch.load(\n            filename, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n        # load the weights into the model\n        set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)\n        if (\n            (getattr(self, \"hf_device_map\", None) is not None)\n            and (len(set(self.hf_device_map.values()).intersection({\"cpu\", \"disk\"})) > 0)\n            and len(self.peft_config) == 1\n        ):\n            device_map = kwargs.get(\"device_map\", \"auto\")\n            max_memory = kwargs.get(\"max_memory\", None)\n            offload_dir = kwargs.get(\"offload_folder\", None)\n            offload_index = kwargs.get(\"offload_index\", None)\n\n            dispatch_model_kwargs = {}\n            # Safety checker for previous `accelerate` versions\n            # `offload_index` was introduced in https://github.com/huggingface/accelerate/pull/873/\n            if \"offload_index\" in inspect.signature(dispatch_model).parameters:\n                dispatch_model_kwargs[\"offload_index\"] = offload_index\n\n            no_split_module_classes = self._no_split_modules\n\n            if device_map != \"sequential\":\n                max_memory = get_balanced_memory(\n                    self,\n                    max_memory=max_memory,\n                    no_split_module_classes=no_split_module_classes,\n                    low_zero=(device_map == \"balanced_low_0\"),\n                )\n            if isinstance(device_map, str):\n                device_map = infer_auto_device_map(\n                    self, max_memory=max_memory, no_split_module_classes=no_split_module_classes\n                )\n            dispatch_model(\n                self,\n                device_map=device_map,\n                offload_dir=offload_dir,\n                **dispatch_model_kwargs,\n            )\n            hook = AlignDevicesHook(io_same_device=True)\n            add_hook_to_module(self.get_base_model(), hook)\n\n        # Set model in evaluation mode to deactivate Dropout modules by default\n        self.eval()\n\n    def set_adapter(self, adapter_name):\n        \"\"\"\n        Sets the active adapter.\n        \"\"\"\n        if adapter_name not in self.peft_config:\n            raise ValueError(f\"Adapter {adapter_name} not found.\")\n        self.active_adapter = adapter_name\n        self.base_model.set_adapter(adapter_name)\n        _set_adapter(self, adapter_name)\n    \n    @property\n    def active_peft_config(self):\n        return self.peft_config[self.active_adapter]\n\nclass Linear8bitLt(bnb.nn.Linear8bitLt, LoraLayer):\n        # Lora implemented in a dense layer\n        def __init__(\n            self,\n            adapter_name,\n            in_features,\n            out_features,\n            r: int = 0,\n            lora_alpha: int = 1,\n            lora_dropout: float = 0.0,\n            **kwargs,\n        ):\n            bnb.nn.Linear8bitLt.__init__(\n                self,\n                in_features,\n                out_features,\n                bias=kwargs.get(\"bias\", True),\n                has_fp16_weights=kwargs.get(\"has_fp16_weights\", True),\n                memory_efficient_backward=kwargs.get(\"memory_efficient_backward\", False),\n                threshold=kwargs.get(\"threshold\", 0.0),\n                index=kwargs.get(\"index\", None),\n            )\n            LoraLayer.__init__(self, in_features=in_features, out_features=out_features)\n\n            # Freezing the pre-trained weight matrix\n            self.weight.requires_grad = False\n            init_lora_weights = kwargs.pop(\"init_lora_weights\", True)\n            self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)\n            self.active_adapter = adapter_name\n\n        def forward(self, x: torch.Tensor):\n            result = super().forward(x)\n\n            if self.disable_adapters or self.active_adapter not in self.lora_A.keys():\n                return result\n            elif self.r[self.active_adapter] > 0:\n                if not torch.is_autocast_enabled():\n                    expected_dtype = result.dtype\n\n                    if x.dtype != torch.float32:\n                        x = x.float()\n                    output = (\n                        self.lora_B[self.active_adapter](\n                            self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n                        ).to(expected_dtype)\n                        * self.scaling[self.active_adapter]\n                    )\n                else:\n                    output = (\n                        self.lora_B[self.active_adapter](\n                            self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n                        )\n                        * self.scaling[self.active_adapter]\n                    )\n                result += output\n            return result\n\nclass ModulesToSaveWrapper(torch.nn.Module):\n    def __init__(self, module_to_save, adapter_name):\n        super().__init__()\n        self.original_module = module_to_save\n        self.modules_to_save = torch.nn.ModuleDict({})\n        self.update(adapter_name)\n        self.active_adapter = adapter_name\n\n    def update(self, adapter_name):\n        self.modules_to_save.update(torch.nn.ModuleDict({adapter_name: copy.deepcopy(self.original_module)}))\n\n    def forward(self, *args, **kwargs):\n        if self.active_adapter not in self.modules_to_save:\n            return self.original_module(*args, **kwargs)\n        return self.modules_to_save[self.active_adapter](*args, **kwargs)\n\ndef mark_only_lora_as_trainable(model: nn.Module, bias: str = \"none\") -> None:\n    for n, p in model.named_parameters():\n        if \"lora_\" not in n:\n            p.requires_grad = False\n    if bias == \"none\":\n        return\n    elif bias == \"all\":\n        for n, p in model.named_parameters():\n            if \"bias\" in n:\n                p.requires_grad = True\n    elif bias == \"lora_only\":\n        for m in model.modules():\n            if isinstance(m, LoraLayer) and hasattr(m, \"bias\") and m.bias is not None:\n                m.bias.requires_grad = True\n    else:\n        raise NotImplementedError\n\ndef _freeze_adapter(model, adapter_name):\n    for n, p in model.named_parameters():\n        if adapter_name in n:\n            p.requires_grad = False\n            \ndef _get_submodules(model, key):\n    parent = model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n    target_name = key.split(\".\")[-1]\n    target = model.get_submodule(key)\n    return parent, target, target_name\n\ndef is_bnb_available():\n    return importlib.util.find_spec(\"bitsandbytes\") is not None\n\n\n\ndef _set_trainable(model, adapter_name):\n    key_list = [key for key, _ in model.named_modules()]\n    for key in key_list:\n        target_module_found = any(key.endswith(target_key) for target_key in model.modules_to_save)\n        if target_module_found:\n            parent, target, target_name = _get_submodules(model, key)\n            if isinstance(target, ModulesToSaveWrapper):\n                target.update(adapter_name)\n            else:\n                for param in target.parameters():\n                    param.requires_grad = True\n                setattr(parent, target_name, ModulesToSaveWrapper(target, adapter_name))\n\ndef get_peft_model_state_dict(model, state_dict=None, adapter_name=\"default\"):\n    config = model.peft_config[adapter_name]\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if config.peft_type in (\"LORA\"):\n        bias = config.bias\n        if bias == \"none\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k}\n        elif bias == \"all\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k or \"bias\" in k}\n        elif bias == \"lora_only\":\n            to_return = {}\n            for k in state_dict:\n                if \"lora_\" in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split(\"lora_\")[0] + \"bias\"\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n        to_return = {k: v for k, v in to_return.items() if ((\"lora_\" in k and adapter_name in k) or (\"bias\" in k))}\n    else:\n        raise NotImplementedError\n    if model.modules_to_save is not None:\n        for key, value in state_dict.items():\n            if any(f\"{module_name}.modules_to_save.{adapter_name}\" in key for module_name in model.modules_to_save):\n                to_return[key.replace(\"modules_to_save.\", \"\")] = value\n\n    to_return = {k.replace(f\".{adapter_name}\", \"\"): v for k, v in to_return.items()}\n    return to_return\n\n\ndef set_peft_model_state_dict(model, peft_model_state_dict, adapter_name=\"default\"):\n    \"\"\"\n    Set the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model.\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\n    \"\"\"\n    config = model.peft_config[adapter_name]\n    state_dict = {}\n    if model.modules_to_save is not None:\n        for key, value in peft_model_state_dict.items():\n            if any(module_name in key for module_name in model.modules_to_save):\n                for module_name in model.modules_to_save:\n                    if module_name in key:\n                        key = key.replace(module_name, f\"{module_name}.modules_to_save.{adapter_name}\")\n                        break\n            state_dict[key] = value\n    else:\n        state_dict = peft_model_state_dict\n\n    if config.peft_type in (\"LORA\"):\n        peft_model_state_dict = {}\n        for k, v in state_dict.items():\n            if \"lora_\" in k:\n                suffix = k.split(\"lora_\")[1]\n                if \".\" in suffix:\n                    suffix_to_replace = \".\".join(suffix.split(\".\")[1:])\n                    k = k.replace(suffix_to_replace, f\"{adapter_name}.{suffix_to_replace}\")\n                else:\n                    k = f\"{k}.{adapter_name}\"\n                peft_model_state_dict[k] = v\n            else:\n                peft_model_state_dict[k] = v\n    else:\n        raise NotImplementedError\n\n    model.load_state_dict(peft_model_state_dict, strict=False)\n    \n\ndef _set_adapter(model, adapter_name):\n    for module in model.modules():\n        if isinstance(module, ModulesToSaveWrapper):\n            module.active_adapter = adapter_name\n            \n            \n\nTRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n    \"t5\": [\"q\", \"v\"],\n    \"mt5\": [\"q\", \"v\"],\n    \"bart\": [\"q_proj\", \"v_proj\"],\n    \"gpt2\": [\"c_attn\"],\n    \"bloom\": [\"query_key_value\"],\n    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n    \"opt\": [\"q_proj\", \"v_proj\"],\n    \"gptj\": [\"q_proj\", \"v_proj\"],\n    \"gpt_neox\": [\"query_key_value\"],\n    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n    \"bert\": [\"query\", \"value\"],\n    \"roberta\": [\"query\", \"value\"],\n    \"xlm-roberta\": [\"query\", \"value\"],\n    \"electra\": [\"query\", \"value\"],\n    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n    \"deberta\": [\"in_proj\"],\n    \"layoutlm\": [\"query\", \"value\"],\n    \"llama\": [\"q_proj\", \"v_proj\"],\n    \"chatglm\": [\"query_key_value\"],\n}","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.206290Z","iopub.execute_input":"2024-02-19T04:56:04.206839Z","iopub.status.idle":"2024-02-19T04:56:04.233813Z","shell.execute_reply.started":"2024-02-19T04:56:04.206795Z","shell.execute_reply":"2024-02-19T04:56:04.232918Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Overwriting lora_model.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile prompt.py\n\nfrom typing import Union\n\nclass Prompter(object):\n    __slots__ = (\"template\")\n\n    def __init__(self, template_name: str = \"\", verbose: bool = False):\n        self.template = {\n            \"description\": \"Template used by Alpaca-LoRA.\",\n            \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n            \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n            \"response_split\": \"### Response:\"}\n\n    def generate_prompt(\n        self,\n        instruction: str,\n        input: Union[None, str] = None,\n        label: Union[None, str] = None,\n    ) -> str:\n        # returns the full prompt from instruction and optional input\n        # if a label (=response, =output) is provided, it's also appended.\n        if input:\n            res = self.template[\"prompt_input\"].format(\n                instruction=instruction, input=input\n            )\n        else:\n            res = self.template[\"prompt_no_input\"].format(\n                instruction=instruction\n            )\n        if label:\n            res = f\"{res}{label}\"\n        return res\n\n    def get_response(self, output: str) -> str:\n        return output.split(self.template[\"response_split\"])[1].strip()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.235468Z","iopub.execute_input":"2024-02-19T04:56:04.235731Z","iopub.status.idle":"2024-02-19T04:56:04.250952Z","shell.execute_reply.started":"2024-02-19T04:56:04.235708Z","shell.execute_reply":"2024-02-19T04:56:04.250095Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing prompt.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile inference.py\n\nimport torch\nfrom transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer, AutoConfig\n\nfrom lora_model import LoraModelForCasualLM\nfrom prompt import Prompter\n\ndef get_response(prompt, tokenizer, model, generation_config, max_new_tokens):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    output = model.generate(\n                input_ids=inputs['input_ids'].cuda(),\n                generation_config=generation_config,\n                max_new_tokens=max_new_tokens,\n                do_sample=True)\n    output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n    return output\n\ndef generate_inference(instruction: str, user_inp: str, model_path:str, lora_weights_path:str):\n    \n    top_k = 40\n    top_p = 128\n    temperature = 0.1\n    num_beams = 1\n    max_new_tokens = 128\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    \n    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n    architecture = config.architectures[0]\n    if \"Llama\" in architecture:\n        print(\"Setting EOS, BOS, UNK, and PAD tokens for LLama tokenizer\")\n        tokenizer.add_special_tokens(\n            {\n                \"eos_token\": \"</s>\",\n                \"bos_token\": \"</s>\",\n                \"unk_token\": \"</s>\",\n            }\n        )\n        tokenizer.pad_token_id = (\n            0  # unk. we want this to be different from the eos token\n        )\n        tokenizer.padding_side = \"left\"\n    \n    model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            device_map=\"auto\",\n            trust_remote_code=True)\n    model = LoraModelForCasualLM.from_pretrained(\n            model,\n            lora_weights_path,\n            torch_dtype=torch.float16,\n            trust_remote_code=True)\n        \n    model.config.pad_token_id = tokenizer.pad_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.eval()\n    \n    generation_config = GenerationConfig(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            num_beams=num_beams)\n    \n    prompter = Prompter()\n    \n   \n    if user_inp.lower().strip() == \"n/a\":\n        user_inp = None\n    prompt = prompter.generate_prompt(instruction, user_inp)\n    output = get_response(prompt, tokenizer, model, generation_config, max_new_tokens)\n    response = prompter.get_response(output)\n    return response\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.252012Z","iopub.execute_input":"2024-02-19T04:56:04.252314Z","iopub.status.idle":"2024-02-19T04:56:04.263052Z","shell.execute_reply.started":"2024-02-19T04:56:04.252280Z","shell.execute_reply":"2024-02-19T04:56:04.262197Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Overwriting inference.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile prepare_data.py\n\nfrom prompt import Prompter\nfrom datasets import load_dataset\nimport random\n\nfrom typing import Union\n\n\n\ndef create_datasets(data_path, size_valid_set, tokenizer, max_length, seed):\n    def tokenize(prompt, add_eos_token=True):\n        result = tokenizer(\n            prompt,\n            truncation=True,\n            max_length=max_length,\n            padding=False,\n            return_tensors=None\n            )\n\n        if (\n            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < max_length\n            and add_eos_token\n        ):\n            \n            result[\"input_ids\"].append(tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    \n    def generate_and_tokenize_prompt(data_point):\n        full_prompt = prompter.generate_prompt(\n            data_point[\"instruction\"],\n            data_point[\"input\"],\n            data_point[\"output\"],\n        )\n        tokenized_full_prompt = tokenize(full_prompt)\n\n        return tokenized_full_prompt\n    \n    prompter = Prompter()\n\n    print(f\"Load dataset....\")\n    dataset = load_dataset('json', split='train', data_files=data_path)\n    dataset = dataset.train_test_split(test_size=size_valid_set, seed=seed)\n\n    train_data = dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n    valid_data = dataset[\"test\"].map(generate_and_tokenize_prompt)\n    \n    train_data.set_format(\"torch\")\n    valid_data.set_format(\"torch\")\n    \n    train_data = train_data.remove_columns(['instruction', 'input', 'output'])\n    valid_data = valid_data.remove_columns(['instruction', 'input', 'output'])\n\n    dataset[\"test\"].to_json('dataset/val_data.json')\n    \n    return train_data, valid_data\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.265219Z","iopub.execute_input":"2024-02-19T04:56:04.265471Z","iopub.status.idle":"2024-02-19T04:56:04.276949Z","shell.execute_reply.started":"2024-02-19T04:56:04.265449Z","shell.execute_reply":"2024-02-19T04:56:04.276081Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Overwriting prepare_data.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile common.py\nimport gdown\ndef download_from_driver(path, location_path):\n    print(f\"Begin download...., path: {path}\")\n    gdown.download(path, location_path, quiet=False, fuzzy=True)\n    print(f\"Completed download!!!: {path}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.277928Z","iopub.execute_input":"2024-02-19T04:56:04.278212Z","iopub.status.idle":"2024-02-19T04:56:04.291561Z","shell.execute_reply.started":"2024-02-19T04:56:04.278184Z","shell.execute_reply":"2024-02-19T04:56:04.290747Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Overwriting common.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile logger_utils.py\nimport logging\n\n\nclass NoReceivedCommandFilter(logging.Filter):\n    def filter(self, record):\n        if 'Received command c' not in record.getMessage():\n            return record.getMessage()\n\n\nclass NoPythonDotEnvFilter(logging.Filter):\n    def filter(self, record):\n        if 'Python-dotenv' not in record.getMessage():\n            return record.getMessage()\n\n\ndef get_logger():\n    logging.getLogger('py4j.java_gateway').setLevel(logging.ERROR)\n    logging.basicConfig(\n        format='%(asctime)s %(levelname)-8s %(message)s', level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    filter_1 = NoReceivedCommandFilter()\n    filter_2 = NoPythonDotEnvFilter()\n    logger.addFilter(filter_1)\n    logger.addFilter(filter_2)\n\n    return logger\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.292605Z","iopub.execute_input":"2024-02-19T04:56:04.292889Z","iopub.status.idle":"2024-02-19T04:56:04.304136Z","shell.execute_reply.started":"2024-02-19T04:56:04.292865Z","shell.execute_reply":"2024-02-19T04:56:04.303278Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Overwriting logger_utils.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile train.py\nimport os\nimport torch\nfrom tqdm import tqdm\n\n\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq\n\nfrom contextlib import nullcontext\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader, SequentialSampler\n\n\nfrom lora_model import LoraModelForCasualLM\nfrom common import download_from_driver\nfrom prepare_data import create_datasets\n\nimport warnings\nwarnings.filterwarnings('ignore')\ntorch.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\n\n\nclass Trainer:\n    def __init__(self,\n                 model,\n                 tokenizer,\n                 gpu_id: int,\n                 is_ddp_training: bool = True,\n                 output_dir: str = 'checkpoints/',\n                 num_epochs: int = 10,\n                 max_length: int = 128,\n                 batch_size: int = 8,\n                 mixed_precision_dtype=None,\n                 gradient_accumulation_steps: int = 16):\n        \"\"\"\n        Initialize the Trainer class.\n\n        Args:\n            model: Pretrained model object.\n            tokenizer: Tokenizer object for text processing.\n            num_epochs: Number of training epochs.\n            max_length: Maximum sequence length.\n            batch_size: Training batch size.\n            gpu_id: GPU ID for training.\n        \"\"\"\n\n        self.num_epochs = num_epochs\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.output_dir = output_dir\n        self.tokenizer = tokenizer\n        self.is_ddp_training = is_ddp_training\n\n        self.gpu_id = gpu_id\n        self.model = model.to(f\"cuda:{self.gpu_id}\")\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n\n        self.mixed_precision_dtype = mixed_precision_dtype\n        self.ctx = None\n        self.gradscaler = None\n\n        # set mixed precision context\n        self.set_mixed_precision_context(mixed_precision_dtype)\n\n    def set_mixed_precision_context(self, mixed_precision_dtype):\n        \n        # TODO: Setup mixed precision training context\n\n        if mixed_precision_dtype is None:\n            \n            # If 'mixed_precision_dtype' is None, use 'nullcontext',\n            self.ctx = None\n\n        else:\n        \n            # TODO Otherwise, use 'torch.amp.autocast' context with the specified dtype, and initialize GradScaler if mixed_precision_dtype is float16.\n            self.ctx = None\n            self.gradscaler = None\n\n    def _set_ddp_training(self):\n\n        # TODO: Initialize the DistributedDataParallel wrapper for the model.\n        # You would need to pass the model and specify the device IDs\n        # and output device for the data parallelism.\n\n        ### YOUR CODE HERE ###\n        self.model = DDP(self.model, device_ids=[self.gpu_id], output_device=self.gpu_id)\n        ### YOUR CODE HERE ###\n\n    def _run_batch(self, batch):\n        \"\"\"\n        Run a single training batch.\n\n        Args:\n            batch: Batch data.\n\n        Returns:\n            Loss value for the batch.\n        \"\"\"\n\n        with self.ctx:\n            outputs = self.model(**batch)\n            loss = outputs.loss / self.gradient_accumulation_steps  # Normalize loss\n        loss_val = loss.item()\n\n        # TODO: If 'mixed_precision_dtype' is torch.float16, you have to modify the backward using the gradscaler.\n        if self.mixed_precision_dtype == torch.float16:\n\n            ### YOUR CODE HERE ###\n            self.gradscaler.scale(loss).backward()\n            ### YOUR CODE HERE ###\n                      \n        else:\n            loss.backward()\n\n        return loss_val\n\n    def _run_epoch(self, train_dataloader, epoch):\n        \"\"\"\n        Run a single training epoch.\n\n        Args:\n            train_loader: Training data loader.\n            epoch: Current epoch number.\n\n        Returns:\n            Total loss value for the epoch.\n        \"\"\"\n\n        epoch_loss = 0\n        self.model.train()\n\n        if _is_master_process():\n            train_progress_bar = tqdm(\n                train_dataloader, desc=f\"Epoch {epoch + 1} [Training]\", position=0, leave=False)\n        else:\n            train_progress_bar = train_dataloader\n\n        # Add counter for gradient accumulation\n        steps = 0\n        self.optimizer.zero_grad()  # Reset gradients at the beginning of each epoch\n        for step, batch in enumerate(train_progress_bar):\n            steps += 1\n            batch = {key: value.to(self.gpu_id)\n                     for key, value in batch.items()}\n            loss = self._run_batch(batch)\n            epoch_loss += loss\n\n            # Perform optimizer step and reset gradients after accumulating enough gradients\n            if steps % self.gradient_accumulation_steps == 0:\n\n                # If 'mixed_precision_dtype' is torch.float16, you have to modify the gradient update step using the gradscaler.\n                if self.mixed_precision_dtype == torch.float16:\n\n                    ### YOUR CODE HERE ###\n                    self.gradscaler.step(self.optimizer)\n                    self.gradscaler.update()\n                    ### YOUR CODE HERE ###\n                    # TODO: optimizer step\n\n                    # TODO: update scaler factor\n\n                    \n                else:\n                    self.optimizer.step()\n                self.optimizer.zero_grad()\n\n                torch.cuda.empty_cache()\n        epoch_loss /= (len(train_dataloader) /\n                       self.gradient_accumulation_steps)\n        return epoch_loss\n\n    def _save_checkpoint(self, epoch):\n        path_dir = f\"{self.output_dir}/epoch_{epoch}\"\n\n        # check path_dir exited\n        if not os.path.exists(path_dir):\n            os.makedirs(path_dir)\n\n        # save checkpoints\n        if self.is_ddp_training and _is_master_process():\n            self.model.module.save_pretrained(f'epoch_{epoch}_checkpoint')\n        else:\n            self.model.save_pretrained(f'epoch_{epoch}_checkpoint')\n\n        print(\"Done saved at\", f'epoch_{epoch}_checkpoint')\n\n    def prepare_dataloader(self, train_dataset, eval_dataset):\n\n        # TODO: Prepare the training DataLoader. Initialize 'DataLoader' with 'train_dataset'\n        # and the appropriate 'batch_size'.\n        # Depending on whether the training is distributed (is_ddp_training),\n        # use 'DistributedSampler' for 'sampler' argument, else use 'None'.\n        # Use 'DataCollatorForSeq2Seq' for 'collate_fn', passing 'tokenizer', padding settings and pad_to_multiple_of to 8, and return_tensors=\"pt\"\n        # Also add drop_last to True.\n\n        ### YOUR CODE HERE ###\n\n        # Initialize training DataLoader\n        data_trainloader = DataLoader(\n         train_dataset,\n         batch_size=self.batch_size,\n         sampler=DistributedSampler(train_dataset) if self.is_ddp_training else None, \n         collate_fn=DataCollatorForSeq2Seq(\n             self.tokenizer,\n             padding=True,\n             pad_to_multiple_of=8,\n             return_tensors=\"pt\"\n         ),\n         drop_last=True\n          )\n\n        # TODO: Prepare the evaluation DataLoader. Initialize 'DataLoader' with 'eval_dataset',\n        # the appropriate 'batch_size', and 'SequentialSampler' for 'sampler'.\n        # Use 'DataCollatorForSeq2Seq' for 'collate_fn', passing 'tokenizer', padding settings and pad_to_multiple_of to 8, and return_tensors=\"pt\".\n        # Also add drop_last to True.\n\n        ### YOUR CODE HERE ###\n\n        # Initialize evaluation DataLoader\n        data_testloader = DataLoader(\n         eval_dataset,\n         batch_size=self.batch_size,\n         sampler=SequentialSampler(eval_dataset),\n         collate_fn=DataCollatorForSeq2Seq(\n             self.tokenizer,\n             padding=True,\n             pad_to_multiple_of=8,\n             return_tensors=\"pt\"\n         ),\n         drop_last=True\n         )\n\n        return data_trainloader, data_testloader\n\n    def _eval(self, eval_dataloader, epoch: int):\n        avg_loss = 0\n        model.eval()\n        if _is_master_process():\n            eval_progress_bar = tqdm(\n                eval_dataloader, desc=f\"Epoch {epoch + 1} [Evaluation]\", position=0, leave=False)\n        else:\n            eval_progress_bar = eval_dataloader\n\n\n        for batch in eval_progress_bar:\n            with self.ctx:\n                with torch.no_grad():\n                    if not self.is_ddp_training:\n                        outputs = self.model(**batch.to(self.gpu_id))\n                    else:\n                        outputs = self.model(**batch)\n            avg_loss += outputs.loss.item()\n        avg_loss = avg_loss/(len(eval_dataloader))\n        return avg_loss\n\n    def run(self, data_path: str, size_valid_set: int = 0.25, seed: int = 123):\n        \"\"\"\n        Run the training process.\n\n        Returns:\n            None\n        \"\"\"\n        train_dataset, eval_dataset = create_datasets(\n            tokenizer=self.tokenizer,\n            max_length=self.max_length,\n            data_path=data_path,\n            size_valid_set=size_valid_set,\n            seed=seed\n        )\n\n        train_dataloader, eval_dataloader = self.prepare_dataloader(\n            train_dataset, eval_dataset)\n\n        if self.is_ddp_training:\n            self._set_ddp_training()\n\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(), lr=learning_rate)\n\n        for epoch in range(self.num_epochs):\n\n            if self.is_ddp_training:\n                train_dataloader.sampler.set_epoch(epoch)\n\n            train_loss = self._run_epoch(train_dataloader, epoch)\n            if self.is_ddp_training:\n                dist.barrier() \n            if _is_master_process() or (epoch == self.num_epochs - 1):\n                eval_loss = self._eval(\n                    eval_dataloader=eval_dataloader, epoch=epoch)\n\n                print(\n                    f\"epoch = {epoch+1} | avg_train_loss = {train_loss} | eval_loss = {eval_loss}\")\n            \n            if _is_master_process():\n                self._save_checkpoint(epoch=epoch+1)\n\ndef load_tokenizer_from_pretrained_model(model_path):\n\n    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n    architecture = config.architectures[0]\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_path, trust_remote_code=True, device_map={\"\": torch.device(f\"cuda:{0}\")})\n    tokenizer.pad_token = tokenizer.eos_token\n    if _is_master_process():\n        print('Completed to load config & tokenizer')\n\n    if \"Llama\" in architecture:\n        if _is_master_process():\n            print(\"Setting EOS, BOS, UNK, and PAD tokens for LLama tokenizer\")\n        tokenizer.add_special_tokens(\n            {\n                \"eos_token\": \"</s>\",\n                \"bos_token\": \"</s>\",\n                \"unk_token\": \"</s>\",\n            }\n        )\n        tokenizer.pad_token_id = (\n            0  # unk. we want this to be different from the eos token\n        )\n\n    return tokenizer\n\n\ndef _is_master_process():\n    ddp_rank = int(os.environ['RANK'])\n    return ddp_rank == 0\n\n\ndef load_pretrained_model(local_rank, model_path: str = \"\"):\n    # TODO: Load a pretrained AutoModelForCausalLM from the 'model_path'.\n    # Make sure to set 'device_map' to '{\"\": torch.device(f\"cuda:{local_rank}\")}' for DDP training\n    # and trust_remote_code=True.\n\n    ### YOUR CODE HERE ###\n\n    # Load pretrained model (AutoModelForCausalLM)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        trust_remote_code=True,\n        device_map={\"\": torch.device(f\"cuda:{local_rank}\")}\n    )\n\n    # TODO: Create a LoraConfig with the parameters: \n    # r=4, lora_alpha=8, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=['lm_head.linear', 'transformer.embd.wte'].\n    # We will then use the config to initialize a LoraModelForCasualLM with the loaded model.\n\n    ### YOUR CODE HERE ###\n\n    # Create LoRA configuration\n    lora_config = LoraConfig(\n        r=4,\n        lora_alpha=8,\n        lora_dropout=0.05, \n        bias=\"none\",    \n        task_type=\"CAUSAL_LM\",  \n        target_modules=['lm_head.linear', 'transformer.embd.wte'] \n    )\n\n    # TODO: Create LoRA model\n    # Apply current model to Lora Model\n    # Create LoRA model\n    model = LoraModelForCasualLM(model, lora_config)  \n\n    if _is_master_process():\n        model.print_trainable_parameters()\n\n    return model\n\n\nif __name__ == \"__main__\":\n    OUTPUT_DIR = \"checkpoints/\"\n\n    backend = \"nccl\"\n    model_path = \"TheBloke/phi-2-GPTQ\"\n    if os.environ.get(\"DEBUG\"):\n        data_path = \"/kaggle/input/cosodulieu/test_data.json\"\n    else:\n        data_path = '/kaggle/input/cosodulieu/alpaca_data.json'\n\n    size_valid_set = 0.15\n    max_length = 128\n    num_epochs = 3\n    batch_size = 2\n    gradient_accumulation_steps = 8\n\n    learning_rate = 3e-4\n    lr_scheduler_type = 'cosine'\n    num_warmup_steps = 100\n    weight_decay = 0.06\n\n    seed = 0\n    log_freq = 1\n    eval_freq = 150\n\n    distributed_strategy = \"ddp\" if os.environ.get(\"ON_DDP\") else \"no\"\n\n    if distributed_strategy == \"ddp\":\n\n        # TODO: Initialize the process group for distributed data parallelism with nccl backend.\n        # After that, you should set the 'local_rank' from the environment variable 'LOCAL_RANK'.\n\n        # Initialize the process group\n\n        ### YOUR CODE HERE ###\n        init_process_group(backend=\"nccl\", init_method=\"env://\")\n        local_rank = int(os.environ['LOCAL_RANK'])\n        ### YOUR CODE HERE ###\n    else:\n        os.environ['RANK'] = '0'\n        local_rank = 0\n\n    # Prepare model\n    model = load_pretrained_model(local_rank, model_path=model_path)\n    \n    # Get tokenizer\n    tokenizer = load_tokenizer_from_pretrained_model(model_path=model_path)\n\n    # prepare trainer\n    trainer = Trainer(\n        model=model,\n        num_epochs=num_epochs,\n        max_length=max_length,\n        batch_size=batch_size,\n        gpu_id=local_rank,\n        \n        mixed_precision_dtype=torch.float16 if os.environ.get(\"ON_MP\") else None,\n        \n        tokenizer=tokenizer,\n        output_dir=OUTPUT_DIR,\n        is_ddp_training=True if distributed_strategy == \"ddp\" else False,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n    )\n\n    # set ddp for wraping model\n    # execute trainer\n    trainer.run(\n        data_path=data_path,\n        size_valid_set=size_valid_set,\n        seed=seed\n    )\n\n    if distributed_strategy == \"ddp\":\n        destroy_process_group()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.305487Z","iopub.execute_input":"2024-02-19T04:56:04.305747Z","iopub.status.idle":"2024-02-19T04:56:04.324610Z","shell.execute_reply.started":"2024-02-19T04:56:04.305725Z","shell.execute_reply":"2024-02-19T04:56:04.323769Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile requirements.txt\n\ngdown\nsentencepiece\ntransformers>=4.28.0\nloralib\nbitsandbytes \nappdirs\ngit+https://github.com/huggingface/accelerate.git\ngit+https://github.com/huggingface/datasets.git\neinops\nauto-gptq\noptimum\ngit+https://github.com/huggingface/peft.git","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.325707Z","iopub.execute_input":"2024-02-19T04:56:04.326013Z","iopub.status.idle":"2024-02-19T04:56:04.338544Z","shell.execute_reply.started":"2024-02-19T04:56:04.325984Z","shell.execute_reply":"2024-02-19T04:56:04.337668Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Overwriting requirements.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:04.339477Z","iopub.execute_input":"2024-02-19T04:56:04.339742Z","iopub.status.idle":"2024-02-19T04:56:58.697476Z","shell.execute_reply.started":"2024-02-19T04:56:04.339720Z","shell.execute_reply":"2024-02-19T04:56:58.696529Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/accelerate.git (from -r requirements.txt (line 8))\n  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-gl0xt4op\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-gl0xt4op\n  Resolved https://github.com/huggingface/accelerate.git to commit 97d2168e5953fe7373a06c69c02c5a00a84d5344\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting git+https://github.com/huggingface/datasets.git (from -r requirements.txt (line 9))\n  Cloning https://github.com/huggingface/datasets.git to /tmp/pip-req-build-1ht8_hga\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/datasets.git /tmp/pip-req-build-1ht8_hga\n  Resolved https://github.com/huggingface/datasets.git to commit bdebf1922663c30744efb8869c86b28f102b84dd\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 13))\n  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-ira3y247\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-ira3y247\n  Resolved https://github.com/huggingface/peft.git to commit 65513e5db4d23935f9fc793eafd70bd0b945da90\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gdown in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (5.1.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.1.99)\nRequirement already satisfied: transformers>=4.28.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.37.0)\nRequirement already satisfied: loralib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.1.2)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.42.0)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.4.4)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.7.0)\nRequirement already satisfied: auto-gptq in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.7.0)\nRequirement already satisfied: optimum in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.17.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 2)) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 2)) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 2)) (4.66.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 4)) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 4)) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 4)) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 4)) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 4)) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 4)) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 4)) (0.4.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r requirements.txt (line 6)) (1.11.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.28.0.dev0->-r requirements.txt (line 8)) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.28.0.dev0->-r requirements.txt (line 8)) (2.1.2)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.1.dev0->-r requirements.txt (line 9)) (15.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.1.dev0->-r requirements.txt (line 9)) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.1.dev0->-r requirements.txt (line 9)) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.1.dev0->-r requirements.txt (line 9)) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.1.dev0->-r requirements.txt (line 9)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.1.dev0->-r requirements.txt (line 9)) (0.70.15)\nRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.1.dev0->-r requirements.txt (line 9)) (3.9.1)\nRequirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (from auto-gptq->-r requirements.txt (line 11)) (1.0.1)\nRequirement already satisfied: gekko in /opt/conda/lib/python3.10/site-packages (from auto-gptq->-r requirements.txt (line 11)) (1.0.6)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum->-r requirements.txt (line 12)) (15.0.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum->-r requirements.txt (line 12)) (1.12)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.28.0->-r requirements.txt (line 4)) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.28.0->-r requirements.txt (line 4)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (2023.11.17)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0.dev0->-r requirements.txt (line 8)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0.dev0->-r requirements.txt (line 8)) (3.1.2)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 12)) (3.20.3)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 2)) (2.5)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum->-r requirements.txt (line 12)) (10.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.1.dev0->-r requirements.txt (line 9)) (2023.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 2)) (1.7.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq->-r requirements.txt (line 11)) (1.16.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum->-r requirements.txt (line 12)) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0.dev0->-r requirements.txt (line 8)) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"!DEBUG=true ON_MP=true python train.py ","metadata":{"execution":{"iopub.status.busy":"2024-02-19T04:56:58.700139Z","iopub.execute_input":"2024-02-19T04:56:58.700450Z","iopub.status.idle":"2024-02-19T04:57:51.154123Z","shell.execute_reply.started":"2024-02-19T04:56:58.700422Z","shell.execute_reply":"2024-02-19T04:57:51.153029Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"2024-02-19 04:57:04.210120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-19 04:57:04.210187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-19 04:57:04.211790: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nconfig.json: 100%|| 1.59k/1.59k [00:00<00:00, 9.89MB/s]\nconfiguration_phi.py: 100%|| 2.03k/2.03k [00:00<00:00, 15.2MB/s]\nA new version of the following files was downloaded from https://huggingface.co/TheBloke/phi-2-GPTQ:\n- configuration_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nmodeling_phi.py: 100%|| 33.4k/33.4k [00:00<00:00, 3.57MB/s]\nA new version of the following files was downloaded from https://huggingface.co/TheBloke/phi-2-GPTQ:\n- modeling_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nmodel.safetensors: 100%|| 1.84G/1.84G [00:33<00:00, 55.1MB/s]\ngeneration_config.json: 100%|| 69.0/69.0 [00:00<00:00, 411kB/s]\ntrainable params: 430080 || all params: 262794240 || trainable%: 0.16365655502951662\ntokenizer_config.json: 100%|| 7.34k/7.34k [00:00<00:00, 25.9MB/s]\nvocab.json: 100%|| 798k/798k [00:00<00:00, 18.3MB/s]\nmerges.txt: 100%|| 456k/456k [00:00<00:00, 14.5MB/s]\ntokenizer.json: 100%|| 2.11M/2.11M [00:00<00:00, 41.5MB/s]\nadded_tokens.json: 100%|| 1.08k/1.08k [00:00<00:00, 6.99MB/s]\nspecial_tokens_map.json: 100%|| 99.0/99.0 [00:00<00:00, 621kB/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nCompleted to load config & tokenizer\nLoad dataset....\nTraceback (most recent call last):\n  File \"/kaggle/working/train.py\", line 443, in <module>\n    trainer.run(\n  File \"/kaggle/working/train.py\", line 269, in run\n    train_dataset, eval_dataset = create_datasets(\n  File \"/kaggle/working/prepare_data.py\", line 46, in create_datasets\n    dataset = load_dataset('json', split='train', data_files=data_path)\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2548, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2220, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1761, in dataset_module_factory\n    ).get_module()\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1129, in get_module\n    data_files = DataFilesDict.from_patterns(\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/data_files.py\", line 693, in from_patterns\n    DataFilesList.from_patterns(\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/data_files.py\", line 598, in from_patterns\n    resolve_pattern(\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/data_files.py\", line 386, in resolve_pattern\n    raise FileNotFoundError(error_msg)\nFileNotFoundError: Unable to find '/kaggle/input/cosodulieu/test_data.json'\n","output_type":"stream"}]}]}